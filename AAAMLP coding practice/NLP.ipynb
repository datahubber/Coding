{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f46dfefa-2991-4978-9d3a-dcd42b7065dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentiment(sentence, pos, neg):\n",
    "    \"\"\"\n",
    "    This function returns the sentiment of a sentence.\n",
    "    :param sentence: sentence, a string\n",
    "    :param pos: set of positive words\n",
    "    :param neg: set of negative words\n",
    "    :return: returns positive, negative or neutral sentiment\n",
    "    \"\"\"\n",
    "\n",
    "    # Split sentence by all whitespaces\n",
    "    sentence = sentence.split()\n",
    "\n",
    "    # Convert the list of words into a set\n",
    "    sentence = set(sentence)\n",
    "\n",
    "    # Count the number of common words with the positive set\n",
    "    num_common_pos = len(sentence.intersection(pos))\n",
    "\n",
    "    # Count the number of common words with the negative set\n",
    "    num_common_neg = len(sentence.intersection(neg))\n",
    "\n",
    "    # Determine sentiment by comparing counts and return the result\n",
    "    if num_common_pos > num_common_neg:\n",
    "        return \"positive\"\n",
    "    if num_common_pos < num_common_neg:\n",
    "        return \"negative\"\n",
    "    return \"neutral\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f10bfc2-7aa8-49b4-987a-4a41e3cb2c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', ',', 'how', 'are', 'you', '?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "sentence = \"hi, how are you?\"\n",
    "sentence.split()\n",
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa54bd63-caa1-48a7-81b1-8aad1aec41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# create a corpus of sentences\n",
    "corpus = [\n",
    " \"hello, how are you?\",\n",
    " \"im getting bored at home. And you? What do you think?\",\n",
    " \"did you know about counts\",\n",
    " \"let's see if this works!\",\n",
    " \"YES!!!!\"\n",
    "]\n",
    "# initialize CountVectorizer\n",
    "ctv = CountVectorizer()\n",
    "# fit the vectorizer on corpus\n",
    "ctv.fit(corpus)\n",
    "corpus_transformed = ctv.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7d628e5-553c-4b58-8f56-4ee29ec9388d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 22)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 22)\t2\n",
      "  (2, 0)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 22)\t1\n",
      "  (3, 12)\t1\n",
      "  (3, 15)\t1\n",
      "  (3, 16)\t1\n",
      "  (3, 18)\t1\n",
      "  (3, 20)\t1\n",
      "  (4, 21)\t1\n"
     ]
    }
   ],
   "source": [
    "print(corpus_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "104b65af-e3eb-4266-9d62-26029f1cdd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 9, 'how': 11, 'are': 2, 'you': 22, 'im': 13, 'getting': 8, 'bored': 4, 'at': 3, 'home': 10, 'and': 1, 'what': 19, 'do': 7, 'think': 17, 'did': 6, 'know': 14, 'about': 0, 'counts': 5, 'let': 15, 'see': 16, 'if': 12, 'this': 18, 'works': 20, 'yes': 21}\n"
     ]
    }
   ],
   "source": [
    "print(ctv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12de6e56-83f4-44dc-9c8d-8330beea0716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 14, ',': 2, 'how': 16, 'are': 7, 'you': 27, '?': 4, 'im': 18, 'getting': 13, 'bored': 9, 'at': 8, 'home': 15, '.': 3, 'and': 6, 'what': 24, 'do': 12, 'think': 22, 'did': 11, 'know': 19, 'about': 5, 'counts': 10, 'let': 20, \"'s\": 1, 'see': 21, 'if': 17, 'this': 23, 'works': 25, '!': 0, 'yes': 26}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "# create a corpus of sentences\n",
    "corpus = [\n",
    " \"hello, how are you?\",\n",
    " \"im getting bored at home. And you? What do you think?\",\n",
    " \"did you know about counts\",\n",
    " \"let's see if this works!\",\n",
    " \"YES!!!!\"\n",
    "]\n",
    "# initialize CountVectorizer with word_tokenize from nltk\n",
    "# as the tokenizer\n",
    "ctv = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "# fit the vectorizer on corpus\n",
    "ctv.fit(corpus)\n",
    "corpus_transformed = ctv.transform(corpus)\n",
    "print(ctv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb250264-6706-49cc-8c87-9e101a224407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Willi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Accuracy = 0.8939\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Willi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "Accuracy = 0.8906\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Willi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2\n",
      "Accuracy = 0.895\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Willi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 3\n",
      "Accuracy = 0.8914\n",
      "\n",
      "Fold: 4\n",
      "Accuracy = 0.8948\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Willi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import linear_model, metrics, model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the training data\n",
    "    df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "    \n",
    "    # Map 'positive' to 1 and 'negative' to 0\n",
    "    df.sentiment = df.sentiment.apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "    \n",
    "    # Create a new column called 'kfold' and fill it with -1\n",
    "    df[\"kfold\"] = -1\n",
    "    \n",
    "    # Randomize the rows of the data\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Fetch labels\n",
    "    y = df.sentiment.values\n",
    "    \n",
    "    # Initiate the kfold class from model_selection module\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    # Fill the new kfold column\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "        df.loc[v_, 'kfold'] = f\n",
    "    \n",
    "    # Iterate over the folds created\n",
    "    for fold_ in range(5):\n",
    "        # Temporary dataframes for train and test\n",
    "        train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
    "        test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
    "        \n",
    "        # Initialize CountVectorizer with NLTK's word_tokenize function as tokenizer\n",
    "        count_vec = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "        \n",
    "        # Fit count_vec on training data reviews\n",
    "        count_vec.fit(train_df.review)\n",
    "        \n",
    "        # Transform training and validation data reviews\n",
    "        xtrain = count_vec.transform(train_df.review)\n",
    "        xtest = count_vec.transform(test_df.review)\n",
    "        \n",
    "        # Initialize logistic regression model\n",
    "        model = linear_model.LogisticRegression()\n",
    "        \n",
    "        # Fit the model on training data reviews and sentiment\n",
    "        model.fit(xtrain, train_df.sentiment)\n",
    "        \n",
    "        # Make predictions on test data\n",
    "        preds = model.predict(xtest)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = metrics.accuracy_score(test_df.sentiment, preds)\n",
    "        \n",
    "        print(f\"Fold: {fold_}\")\n",
    "        print(f\"Accuracy = {accuracy}\")\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79314de3-10d3-4dca-9272-0855f4b4127e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Accuracy = 0.8431\n",
      "\n",
      "Fold: 1\n",
      "Accuracy = 0.8494\n",
      "\n",
      "Fold: 2\n",
      "Accuracy = 0.8413\n",
      "\n",
      "Fold: 3\n",
      "Accuracy = 0.8321\n",
      "\n",
      "Fold: 4\n",
      "Accuracy = 0.8532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import what we need\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the training data\n",
    "    df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "    \n",
    "    # Map 'positive' to 1 and 'negative' to 0\n",
    "    df.sentiment = df.sentiment.apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "    \n",
    "    # Create a new column called 'kfold' and fill it with -1\n",
    "    df[\"kfold\"] = -1\n",
    "    \n",
    "    # Randomize the rows of the data\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Fetch labels\n",
    "    y = df.sentiment.values\n",
    "    \n",
    "    # Initiate the kfold class from model_selection module\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    # Fill the new kfold column\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "        df.loc[v_, 'kfold'] = f\n",
    "    \n",
    "    # Iterate over the folds created\n",
    "    for fold_ in range(5):\n",
    "        # Temporary dataframes for train and test\n",
    "        train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
    "        test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
    "        \n",
    "        # Initialize CountVectorizer with NLTK's word_tokenize function as tokenizer\n",
    "        count_vec = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "        \n",
    "        # Fit count_vec on training data reviews\n",
    "        count_vec.fit(train_df.review)\n",
    "        \n",
    "        # Transform training and validation data reviews\n",
    "        xtrain = count_vec.transform(train_df.review)\n",
    "        xtest = count_vec.transform(test_df.review)\n",
    "        \n",
    "        # initialize naive bayes model\n",
    "        model = naive_bayes.MultinomialNB()\n",
    "        \n",
    "        # Fit the model on training data reviews and sentiment\n",
    "        model.fit(xtrain, train_df.sentiment)\n",
    "        \n",
    "        # Make predictions on test data\n",
    "        preds = model.predict(xtest)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = metrics.accuracy_score(test_df.sentiment, preds)\n",
    "        \n",
    "        print(f\"Fold: {fold_}\")\n",
    "        print(f\"Accuracy = {accuracy}\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "097291c6-4645-4b80-bc48-a00af3843129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 27)\t0.2965698850220162\n",
      "  (0, 16)\t0.4428321995085722\n",
      "  (0, 14)\t0.4428321995085722\n",
      "  (0, 7)\t0.4428321995085722\n",
      "  (0, 4)\t0.35727423026525224\n",
      "  (0, 2)\t0.4428321995085722\n",
      "  (1, 27)\t0.35299699146792735\n",
      "  (1, 24)\t0.2635440111190765\n",
      "  (1, 22)\t0.2635440111190765\n",
      "  (1, 18)\t0.2635440111190765\n",
      "  (1, 15)\t0.2635440111190765\n",
      "  (1, 13)\t0.2635440111190765\n",
      "  (1, 12)\t0.2635440111190765\n",
      "  (1, 9)\t0.2635440111190765\n",
      "  (1, 8)\t0.2635440111190765\n",
      "  (1, 6)\t0.2635440111190765\n",
      "  (1, 4)\t0.42525129752567803\n",
      "  (1, 3)\t0.2635440111190765\n",
      "  (2, 27)\t0.31752680284846835\n",
      "  (2, 19)\t0.4741246485558491\n",
      "  (2, 11)\t0.4741246485558491\n",
      "  (2, 10)\t0.4741246485558491\n",
      "  (2, 5)\t0.4741246485558491\n",
      "  (3, 25)\t0.38775666010579296\n",
      "  (3, 23)\t0.38775666010579296\n",
      "  (3, 21)\t0.38775666010579296\n",
      "  (3, 20)\t0.38775666010579296\n",
      "  (3, 17)\t0.38775666010579296\n",
      "  (3, 1)\t0.38775666010579296\n",
      "  (3, 0)\t0.3128396318588854\n",
      "  (4, 26)\t0.2959842226518677\n",
      "  (4, 0)\t0.9551928286692534\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "# create a corpus of sentences\n",
    "corpus = [\n",
    "    \"hello, how are you?\",\n",
    " \"im getting bored at home. And you? What do you think?\",\n",
    " \"did you know about counts\",\n",
    " \"let's see if this works!\",\n",
    " \"YES!!!!\"\n",
    "]\n",
    "# initialize TfidfVectorizer with word_tokenize from nltk\n",
    "# as the tokenizer\n",
    "tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "# fit the vectorizer on corpus\n",
    "tfv.fit(corpus)\n",
    "corpus_transformed = tfv.transform(corpus)\n",
    "print(corpus_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a331174-1644-4c0e-bbcd-a8696b76b298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Accuracy = 0.8987\n",
      "\n",
      "Fold: 1\n",
      "Accuracy = 0.8973\n",
      "\n",
      "Fold: 2\n",
      "Accuracy = 0.8925\n",
      "\n",
      "Fold: 3\n",
      "Accuracy = 0.8963\n",
      "\n",
      "Fold: 4\n",
      "Accuracy = 0.899\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import what we need\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import linear_model, metrics, model_selection\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the training data\n",
    "    df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "    \n",
    "    # Map 'positive' to 1 and 'negative' to 0\n",
    "    df.sentiment = df.sentiment.apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "    \n",
    "    # Create a new column called 'kfold' and fill it with -1\n",
    "    df[\"kfold\"] = -1\n",
    "    \n",
    "    # Randomize the rows of the data\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Fetch labels\n",
    "    y = df.sentiment.values\n",
    "    \n",
    "    # Initiate the kfold class from model_selection module\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    # Fill the new kfold column\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "        df.loc[v_, 'kfold'] = f\n",
    "    \n",
    "    # Iterate over the folds created\n",
    "    for fold_ in range(5):\n",
    "        # Temporary dataframes for train and test\n",
    "        train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
    "        test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
    "        \n",
    "        # Initialize TfidfVectorizer with NLTK's word_tokenize function as tokenizer\n",
    "        tfidf_vec = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "        \n",
    "        # Fit tfidf_vec on training data reviews\n",
    "        tfidf_vec.fit(train_df.review)\n",
    "        \n",
    "        # Transform training and validation data reviews\n",
    "        xtrain = tfidf_vec.transform(train_df.review)\n",
    "        xtest = tfidf_vec.transform(test_df.review)\n",
    "        \n",
    "        # Initialize logistic regression model\n",
    "        model = linear_model.LogisticRegression()\n",
    "        \n",
    "        # Fit the model on training data reviews and sentiment\n",
    "        model.fit(xtrain, train_df.sentiment)\n",
    "        \n",
    "        # Make predictions on test data\n",
    "        preds = model.predict(xtest)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = metrics.accuracy_score(test_df.sentiment, preds)\n",
    "        \n",
    "        print(f\"Fold: {fold_}\")\n",
    "        print(f\"Accuracy = {accuracy}\")\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fa3f9f5-af98-4d7c-aa28-c6ef65abc764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hi', ',', 'how'), (',', 'how', 'are'), ('how', 'are', 'you'), ('are', 'you', '?')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "# let's see 3 grams\n",
    "N = 3\n",
    "# input sentence\n",
    "sentence = \"hi, how are you?\"\n",
    "# tokenized sentence\n",
    "tokenized_sentence = word_tokenize(sentence)\n",
    "# generate n_grams\n",
    "n_grams = list(ngrams(tokenized_sentence, N))\n",
    "print(n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9c56fa4-d94e-42b2-9730-b255270224f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(\n",
    " tokenizer=word_tokenize,\n",
    " token_pattern=None,\n",
    " ngram_range=(1, 3)\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "467383b1-b03e-4a8d-b6cf-dcbe2424e18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word=fishing\n",
      "stemmed_word=fish\n",
      "lemma=fishing\n",
      "\n",
      "word=fishes\n",
      "stemmed_word=fish\n",
      "lemma=fish\n",
      "\n",
      "word=fished\n",
      "stemmed_word=fish\n",
      "lemma=fished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# initialize stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "words = [\"fishing\", \"fishes\", \"fished\"]\n",
    "for word in words:\n",
    " print(f\"word={word}\")\n",
    " print(f\"stemmed_word={stemmer.stem(word)}\")\n",
    " print(f\"lemma={lemmatizer.lemmatize(word)}\")\n",
    " print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f5942d7-7765-4a89-b360-7af676374d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Willi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca13ce9f-9284-4589-a28f-790c2d868ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', ',', '.', 'a', 'and']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a corpus of sentences\n",
    "# Reading only 10k samples from training data for this example\n",
    "corpus = pd.read_csv(\"IMDB Dataset.csv\", nrows=10000)\n",
    "corpus = corpus.review.values\n",
    "\n",
    "# Initialize TfidfVectorizer with word_tokenize from nltk as the tokenizer\n",
    "tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "\n",
    "# Fit the vectorizer on the corpus\n",
    "tfv.fit(corpus)\n",
    "\n",
    "# Transform the corpus using tfidf\n",
    "corpus_transformed = tfv.transform(corpus)\n",
    "\n",
    "# Initialize SVD with 10 components\n",
    "svd = decomposition.TruncatedSVD(n_components=10)\n",
    "\n",
    "# Fit SVD\n",
    "corpus_svd = svd.fit(corpus_transformed)\n",
    "\n",
    "# Choose the first sample and create a dictionary\n",
    "# of feature names and their scores from SVD\n",
    "# You can change the sample_index variable to\n",
    "# get dictionary for any other sample\n",
    "sample_index = 0\n",
    "feature_scores = dict(\n",
    "    zip(\n",
    "        tfv.get_feature_names_out(),\n",
    "        corpus_svd.components_[sample_index]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Once we have the dictionary, we can now\n",
    "# sort it in decreasing order and get the\n",
    "# top N topics\n",
    "N = 5\n",
    "print(sorted(feature_scores, key=feature_scores.get, reverse=True)[:N])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac2b5e4d-ed7f-4ae5-97e1-9971643f662b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', ',', '.', 'a', 'and']\n",
      "['br', '<', '>', '/', '-']\n",
      "['i', 'movie', '!', 'it', 'was']\n",
      "[',', '!', \"''\", '``', 'you']\n",
      "['!', 'the', \"''\", '``', '...']\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "for sample_index in range(5):\n",
    "    feature_scores = dict(\n",
    "        zip(\n",
    "            tfv.get_feature_names_out(),\n",
    "            corpus_svd.components_[sample_index]\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        sorted(\n",
    "            feature_scores,\n",
    "            key=feature_scores.get,\n",
    "            reverse=True\n",
    "        )[:N]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd9c8717-568f-4ebe-a166-dedf2a4cf5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "def clean_text(s):\n",
    " \"\"\"\n",
    " This function cleans the text a bit\n",
    " :param s: string\n",
    "  :return: cleaned string\n",
    " \"\"\"\n",
    " # split by all whitespaces\n",
    " s = s.split()\n",
    "\n",
    " # join tokens by single space\n",
    " # why we do this?\n",
    " # this will remove all kinds of weird space\n",
    " # \"hi. how are you\" becomes\n",
    " # \"hi. how are you\"\n",
    " s = \" \".join(s)\n",
    "\n",
    " # remove all punctuations using regex and string module\n",
    " s = re.sub(f'[{re.escape(string.punctuation)}]', '', s)\n",
    "\n",
    " # you can add more cleaning here if you want\n",
    " # and then return the cleaned string\n",
    " return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7364506e-85c9-4555-89c7-563a7c29b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sentence_to_vec(s, embedding_dict, stop_words, tokenizer):\n",
    "    \"\"\"\n",
    "    Given a sentence and other information,\n",
    "    this function returns embedding for the whole sentence\n",
    "    :param s: sentence, string\n",
    "    :param embedding_dict: dictionary word:vector\n",
    "    :param stop_words: list of stop words, if any\n",
    "    :param tokenizer: a tokenization function\n",
    "    \"\"\"\n",
    "    # convert sentence to string and lowercase it\n",
    "    words = str(s).lower()\n",
    "\n",
    "    # tokenize the sentence\n",
    "    words = tokenizer(words)\n",
    "\n",
    "    # remove stop word tokens\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "\n",
    "    # keep only alpha-numeric tokens\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "\n",
    "    # initialize empty list to store embeddings\n",
    "    M = []\n",
    "    for w in words:\n",
    "        # for every word, fetch the embedding from\n",
    "        # the dictionary and append to list of\n",
    "        # embeddings\n",
    "        if w in embedding_dict:\n",
    "            M.append(embedding_dict[w])\n",
    "\n",
    "    # if we don't have any vectors, return zeros\n",
    "    if len(M) == 0:\n",
    "        return np.zeros(300)\n",
    "\n",
    "    # convert list of embeddings to array\n",
    "    M = np.array(M)\n",
    "\n",
    "    # calculate sum over axis=0\n",
    "    v = M.sum(axis=0)\n",
    "\n",
    "    # return normalized vector\n",
    "    return v / np.sqrt((v ** 2).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ce8be64-0442-4da6-969e-282e513bf5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings\n",
      "Creating sentence vectors\n",
      "Training fold: 0\n",
      "Accuracy = 0.8608\n",
      "\n",
      "Training fold: 1\n",
      "Accuracy = 0.8581\n",
      "\n",
      "Training fold: 2\n",
      "Accuracy = 0.8582\n",
      "\n",
      "Training fold: 3\n",
      "Accuracy = 0.8558\n",
      "\n",
      "Training fold: 4\n",
      "Accuracy = 0.8624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fasttext.py\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def load_vectors(fname):\n",
    "    # taken from: https://fasttext.cc/docs/en/english-vectors.html\n",
    "    fin = io.open(\n",
    "        fname,\n",
    "        'r',\n",
    "        encoding='utf-8',\n",
    "        newline='\\n',\n",
    "        errors='ignore'\n",
    "    )\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = list(map(float, tokens[1:]))\n",
    "    return data\n",
    "\n",
    "def sentence_to_vec(s, embedding_dict, stop_words, tokenizer):\n",
    "    \"\"\"\n",
    "    Given a sentence and other information,\n",
    "    this function returns embedding for the whole sentence\n",
    "    :param s: sentence, string\n",
    "    :param embedding_dict: dictionary word:vector\n",
    "    :param stop_words: list of stop words, if any\n",
    "    :param tokenizer: a tokenization function\n",
    "    \"\"\"\n",
    "    # convert sentence to string and lowercase it\n",
    "    words = str(s).lower()\n",
    "\n",
    "    # tokenize the sentence\n",
    "    words = tokenizer(words)\n",
    "\n",
    "    # remove stop word tokens\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "\n",
    "    # keep only alpha-numeric tokens\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "\n",
    "    # initialize empty list to store embeddings\n",
    "    M = []\n",
    "    for w in words:\n",
    "        # for every word, fetch the embedding from\n",
    "        # the dictionary and append to list of\n",
    "        # embeddings\n",
    "        if w in embedding_dict:\n",
    "            M.append(embedding_dict[w])\n",
    "\n",
    "    # if we don't have any vectors, return zeros\n",
    "    if len(M) == 0:\n",
    "        return np.zeros(300)\n",
    "\n",
    "    # convert list of embeddings to array\n",
    "    M = np.array(M)\n",
    "\n",
    "    # calculate sum over axis=0\n",
    "    v = M.sum(axis=0)\n",
    "\n",
    "    # return normalized vector\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # read the training data\n",
    "    df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "    # map positive to 1 and negative to 0\n",
    "    df.sentiment = df.sentiment.apply(\n",
    "        lambda x: 1 if x == \"positive\" else 0\n",
    "    )\n",
    "    # the next step is to randomize the rows of the data\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    # load embeddings into memory\n",
    "    print(\"Loading embeddings\")\n",
    "    embeddings = load_vectors(\"crawl-300d-2M.vec/crawl-300d-2M.vec\")\n",
    "    # create sentence embeddings\n",
    "    print(\"Creating sentence vectors\")\n",
    "    vectors = []\n",
    "    for review in df.review.values:\n",
    "        vectors.append(\n",
    "            sentence_to_vec(\n",
    "                s=review,\n",
    "                embedding_dict=embeddings,\n",
    "                stop_words=[],\n",
    "                tokenizer=word_tokenize\n",
    "            )\n",
    "        )\n",
    "\n",
    "    vectors = np.array(vectors)\n",
    "    # fetch labels\n",
    "    y = df.sentiment.values\n",
    "\n",
    "    # initiate the kfold class from model_selection module\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # fill the new kfold column\n",
    "    for fold_, (t_, v_) in enumerate(kf.split(X=vectors, y=y)):\n",
    "        print(f\"Training fold: {fold_}\")\n",
    "        # temporary dataframes for train and test\n",
    "        xtrain = vectors[t_, :]\n",
    "        ytrain = y[t_]\n",
    "        xtest = vectors[v_, :]\n",
    "        ytest = y[v_]\n",
    "        # initialize logistic regression model\n",
    "        model = linear_model.LogisticRegression()\n",
    "        # fit the model on training data reviews and sentiment\n",
    "        model.fit(xtrain, ytrain)\n",
    "        # make predictions on test data\n",
    "        # threshold for predictions is 0.5\n",
    "        preds = model.predict(xtest)\n",
    "        # calculate accuracy\n",
    "        accuracy = metrics.accuracy_score(ytest, preds)\n",
    "        print(f\"Accuracy = {accuracy}\")\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "782c5767-1e00-4788-86f3-8c4d8ebbbfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_folds.py\n",
    "# import pandas and model_selection module of scikit-learn\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read training data\n",
    "    df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "    # map positive to 1 and negative to 0\n",
    "    df.sentiment = df.sentiment.apply(\n",
    "        lambda x: 1 if x == \"positive\" else 0\n",
    "    )\n",
    "    # we create a new column called kfold and fill it with -1\n",
    "    df[\"kfold\"] = -1\n",
    "    # the next step is to randomize the rows of the data\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # fetch labels\n",
    "    y = df.sentiment.values\n",
    "\n",
    "    # initiate the kfold class from model_selection module\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # fill the new kfold column\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "        df.loc[v_, 'kfold'] = f\n",
    "\n",
    "    # save the new csv with kfold column\n",
    "    df.to_csv(\"imdb_folds.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d84a75b1-002b-4a87-ae2f-dbb90482a470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.py\n",
    "import torch\n",
    "\n",
    "class IMDBDataset:\n",
    "    def __init__(self, reviews, targets):\n",
    "        \"\"\"\n",
    "        :param reviews: this is a numpy array\n",
    "        :param targets: a vector, numpy array\n",
    "        \"\"\"\n",
    "        self.reviews = reviews\n",
    "        self.target = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        # returns length of the dataset\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # for any given item, which is an int,\n",
    "        # return review and targets as torch tensor\n",
    "        # item is the index of the item in concern\n",
    "        review = self.reviews[item, :]\n",
    "        target = self.target[item]\n",
    "        return {\n",
    "            \"review\": torch.tensor(review, dtype=torch.long),\n",
    "            \"target\": torch.tensor(target, dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11ae1026-a96d-4e36-b92e-4804787e3f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        \"\"\"\n",
    "        :param embedding_matrix: numpy array with vectors for all words\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        # number of words = number of rows in embedding matrix\n",
    "        num_words = embedding_matrix.shape[0]\n",
    "        # dimension of embedding is num of columns in the matrix\n",
    "        embed_dim = embedding_matrix.shape[1]\n",
    "        # we define an input embedding layer\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=num_words,\n",
    "            embedding_dim=embed_dim\n",
    "        )\n",
    "        # embedding matrix is used as weights of\n",
    "        # the embedding layer\n",
    "        self.embedding.weight = nn.Parameter(\n",
    "            torch.tensor(\n",
    "                embedding_matrix,\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "        )\n",
    "        # we don't want to train the pretrained embeddings\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        # a simple bidirectional LSTM with\n",
    "        # hidden size of 128\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            128,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        # output layer which is a linear layer\n",
    "        # we have only one output\n",
    "        # input (512) = 128 + 128 for mean and same for max pooling\n",
    "        self.out = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pass data through embedding layer\n",
    "        # the input is just the tokens\n",
    "        x = self.embedding(x)\n",
    "        # move embedding output to lstm\n",
    "        x, _ = self.lstm(x)\n",
    "        # apply mean and max pooling on lstm output\n",
    "        avg_pool = torch.mean(x, 1)\n",
    "        max_pool, _ = torch.max(x, 1)\n",
    "\n",
    "        # concatenate mean and max pooling\n",
    "        # this is why size is 512\n",
    "        # 128 for each direction = 256\n",
    "        # avg_pool = 256 and max_pool = 256\n",
    "        out = torch.cat((avg_pool, max_pool), 1)\n",
    "        # pass through the output layer and return the output\n",
    "        out = self.out(out)\n",
    "        # return linear output\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8d27dda-d69d-40dd-a032-e1a0c1900bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def train(data_loader, model, optimizer, device):\n",
    "    \"\"\"\n",
    "    This is the main training function that trains model\n",
    "    for one epoch\n",
    "    :param data_loader: this is the torch dataloader\n",
    "    :param model: model (lstm model)\n",
    "    :param optimizer: torch optimizer, e.g. adam, sgd, etc.\n",
    "    :param device: this can be \"cuda\" or \"cpu\"\n",
    "    \"\"\"\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "    # go through batches of data in data loader\n",
    "    for data in data_loader:\n",
    "        # fetch review and target from the dict\n",
    "        reviews = data[\"review\"]\n",
    "        targets = data[\"target\"]\n",
    "        # move the data to device that we want to use\n",
    "        reviews = reviews.to(device, dtype=torch.long)\n",
    "        targets = targets.to(device, dtype=torch.float)\n",
    "        # clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # make predictions from the model\n",
    "        predictions = model(reviews)\n",
    "        # calculate the loss\n",
    "        loss = nn.BCEWithLogitsLoss()(\n",
    "            predictions,\n",
    "            targets.view(-1, 1)\n",
    "        )\n",
    "        # compute gradient of loss w.r.t.\n",
    "        # all parameters of the model that are trainable\n",
    "        loss.backward()\n",
    "        # single optimization step\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(data_loader, model, device):\n",
    "    # initialize empty lists to store predictions\n",
    "    # and targets\n",
    "    final_predictions = []\n",
    "    final_targets = []\n",
    "    # put the model in eval mode\n",
    "    model.eval()\n",
    "    # disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            reviews = data[\"review\"]\n",
    "            targets = data[\"target\"]\n",
    "            reviews = reviews.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.float)\n",
    "            # make predictions\n",
    "            predictions = model(reviews)\n",
    "            # move predictions and targets to list\n",
    "            # we need to move predictions and targets to cpu too\n",
    "            predictions = predictions.cpu().numpy().tolist()\n",
    "            targets = data[\"target\"].cpu().numpy().tolist()\n",
    "            final_predictions.extend(predictions)\n",
    "            final_targets.extend(targets)\n",
    "    # return final predictions and targets\n",
    "    return final_predictions, final_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3c69a6-27bc-47b5-a702-dfe0c7e6e676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/path/to/directory/containing/engine/')\n",
    "import engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46e4b91d-4c63-46b0-8ca2-52630cf96116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lstm\n",
      "  Downloading lstm-0.1.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Downloading lstm-0.1.0-py3-none-any.whl (3.6 kB)\n",
      "Installing collected packages: lstm\n",
      "Successfully installed lstm-0.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "823ec7c2-ba41-4b18-b9a7-e47f82f92120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_embeddings(word_index, embedding_file, vector_length=300):\n",
    "    \"\"\"\n",
    "    Creates an embedding matrix from a pre-trained embeddings file.\n",
    "    :param word_index: Dictionary mapping words to their index in the model.\n",
    "    :param embedding_file: Path to the file containing pre-trained word vectors.\n",
    "    :param vector_length: Length of each word vector.\n",
    "    :return: A matrix of shape (number of words + 1, vector_length) with embedding vectors.\n",
    "    \"\"\"\n",
    "    max_features = len(word_index) + 1  # +1 for the zero padding\n",
    "    embeddings_index = {}\n",
    "\n",
    "    # Load embeddings from file\n",
    "    with open(embedding_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            if word in word_index or word.capitalize() in word_index or word.upper() in word_index:\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "\n",
    "    # Create an embedding matrix\n",
    "    embedding_matrix = np.zeros((max_features, vector_length))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        # Check various forms of the word\n",
    "        embedding_vector = embeddings_index.get(word) \\\n",
    "                        or embeddings_index.get(word.capitalize()) \\\n",
    "                        or embeddings_index.get(word.upper())\n",
    "        \n",
    "        if embedding_vector is not None and len(embedding_vector) == vector_length:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45303866-7054-4f07-998f-cc7cc468e625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
